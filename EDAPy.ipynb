{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b579ae6-ea67-4ddb-b5a5-d406e744b220",
   "metadata": {
    "id": "7b579ae6-ea67-4ddb-b5a5-d406e744b220"
   },
   "source": [
    "# Exploratory Data Analysis (EDA) with Python\n",
    "### \"Tidy datasets are all alike, but every messy dataset is messy in its own way\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c45c5",
   "metadata": {
    "id": "5b2c45c5"
   },
   "source": [
    "## Welcome!\n",
    "\n",
    "This workshop aims at giving you a starter to independently work with your data throughout the *“data analysis lifecycle”* — from **importing**, **cleaning**, **visualising** and **summarising** the data to **analysing**.\n",
    "\n",
    "<img src=\"https://github.com/Sydney-Informatics-Hub/EDAinPy/blob/tomm-updates/fig/data_loop.png?raw=1\" width=\"50%\"/>\n",
    "\n",
    "To follow this tutorial, you need some familiarity with common libraries such as **pandas**, **matplotlib** and **seaborn**.\n",
    "\n",
    "**Now let’s get started!**\n",
    "\n",
    "Incorrect or inconsistent data leads to false conclusions. And so, how well you clean and understand the data has a high impact on the quality of the results.\n",
    "\n",
    "In fact, a simple algorithm can outweigh a complex one just because it was given enough and high-quality data.\n",
    "\n",
    "For these reasons, it is important to have a step-by-step guideline, a cheat sheet, that walks through the quality checks to be applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf10e0",
   "metadata": {
    "id": "7abf10e0"
   },
   "source": [
    "***\n",
    "## ℹ️ **Learning objectives**\n",
    "\n",
    "- Understand data-type constraints, checks and the role of data cleaning techniques in standardising data formats, handling missing values, and addressing outliers;\n",
    "- Develop exploratory data analysis skills to visualise data distributions, detect outliers, and explore relationships between variables.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb0cae",
   "metadata": {
    "id": "d4eb0cae"
   },
   "source": [
    "## The dataset\n",
    "\n",
    "The *Stack Overflow Developer Survey* is a highly anticipated annual event that provides valuable insights into the preferences, trends, and experiences of developers worldwide. In May 2023, over 90,000 developers participated in the survey, sharing their perspectives on learning methods, preferred tools, and salary trends. The [latest dataset 2023](https://survey.stackoverflow.co/2023/) is available [here](https://insights.stackoverflow.com/survey/).\n",
    "\n",
    "<a href=\"https://survey.stackoverflow.co/2023/\">\n",
    "<img src=\"https://github.com/Sydney-Informatics-Hub/EDAinPy/blob/tomm-updates/fig/so2023.png?raw=1\" width=\"50%\"/>\n",
    "</a>\n",
    "\n",
    "This is the voice of the developer. Analysts, IT leaders, reporters, and other developers turn to this report to stay up to date with the evolving developer experience, technologies that are rising or falling in favor, and to understand where tech might be going next.\n",
    "\n",
    "In the early stages of EDA you are encouraged to explore every idea that comes to mind, which is usually guided by your research question. While some of the early ideas will prove fruitful, others may lead to dead ends. As your exploration continues, you will narrow down a few insightful findings that you'll eventually write up and communicate to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493de33",
   "metadata": {
    "id": "1493de33"
   },
   "source": [
    "## Table of Contents\n",
    "1. [Import your libraries and data](#basics)\n",
    "2. [Data semantics](#semantics)\n",
    "3. [Inspection and cleaning](#cleaning)\n",
    "4. [In-record & cross-dataset errors](#errors)\n",
    "5. [Duplicates](#duplicates)\n",
    "6. [Standardise](#standardise)\n",
    "7. [Categorical data](#categorical)\n",
    "8. [Select Variables and Filter Observations](#select_filter)\n",
    "9. [Unusual values](#unusual_values)\n",
    "    - [Outliers](#outliers)\n",
    "    - [Missing values](#missing)\n",
    "        - [Drop](#drop)\n",
    "        - [Impute](#impute)\n",
    "10. [Relationship](#relationship)\n",
    "    - [Correlation](#correlation)\n",
    "11. [Verifying](#verifying)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Completeness](#completeness)\n",
    "    - [Consistency](#consistency)\n",
    "    - [Uniformity](#uniformity)\n",
    "    - [Reporting](#reporting)\n",
    "12. [What's next](#next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e94c5",
   "metadata": {
    "id": "771e94c5"
   },
   "source": [
    "<a id='basics'></a>\n",
    "## Import your libraries and your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429fa2b",
   "metadata": {},
   "source": [
    "We must first ensure that we have all the Python libraries we will use installed and available to us in this Google Colab environment. This first cell will take care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jHGbyLM-xdZ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHGbyLM-xdZ7",
    "outputId": "bdc2475a-1fa0-4961-9b76-e5f8fc417549"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Sydney-Informatics-Hub/EDAinPy.git\n",
    "!pip install -r EDAinPy/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611897e",
   "metadata": {
    "id": "5611897e"
   },
   "source": [
    "It is common practice to load all the packages and modules that we will use in our first code cell. In this workshop we will _also_ import packages and modules as we use them, just so that is clear where particular tools come from, but you would not do this generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a14c75-50f4-4e99-8380-fd40c4828791",
   "metadata": {
    "id": "32a14c75-50f4-4e99-8380-fd40c4828791"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests #to download the file from url\n",
    "import zipfile #to extract the zip file\n",
    "import pandas as pd\n",
    "import janitor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimpy import skim, clean_columns\n",
    "from itables import show\n",
    "import matplotlib.ticker as mtick\n",
    "import missingno as msno\n",
    "from ydata_profiling import ProfileReport\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c9eae",
   "metadata": {},
   "source": [
    "Next we will download our data. The data is contained within a ZIP file that you can download directly from the Stack Overflow website [here](https://cdn.stackoverflow.co/files/jo7n4k8s/production/49915bfd46d0902c3564fd9a06b509d08a20488c.zip/stack-overflow-developer-survey-2023.zip). Once downloaded, the CSV (comma separated values) file with the survey results can be extracted from the ZIP file, and manually uploaded into this Google Colab environment using the menus on the left.\n",
    "\n",
    "Alternatively, the code block below automates this task. It checks to see if the ZIP file has already been downloaded and, if not, requests it from the website and saves it. It then checks to see if the CSV file has already been extracted from the ZIP file and, if not, extracts all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a84c6-3ef0-4116-8baf-79790d9a1031",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "535a84c6-3ef0-4116-8baf-79790d9a1031",
    "outputId": "4a58c996-d076-480f-e6a5-6f74badcd7b1"
   },
   "outputs": [],
   "source": [
    "# Define the destination file\n",
    "destfile = 'stack-overflow-developer-survey-2023.zip'\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not os.path.exists(destfile):\n",
    "    url = 'https://cdn.stackoverflow.co/files/jo7n4k8s/production/49915bfd46d0902c3564fd9a06b509d08a20488c.zip/stack-overflow-developer-survey-2023.zip'\n",
    "    response = requests.get(url) #sends a HTTP GET request to the specified URL to download the file\n",
    "    with open(destfile, 'wb') as file: #write-binary mode\n",
    "        file.write(response.content)\n",
    "\n",
    "# Extract the files from the zip if they haven't been already\n",
    "data_file = \"survey_results_public.csv\"\n",
    "if not os.path.exists(data_file):\n",
    "    with zipfile.ZipFile(destfile, 'r') as z: #opens the zip file in read mode and assigns the opened zip file object to the variable z\n",
    "        #the with statement ensures that z.close() is called automatically when the block inside the with statement is exited\n",
    "        z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8020242",
   "metadata": {
    "id": "d8020242"
   },
   "source": [
    "Now, we will load in the CSV file into a Pandas DataFrame and store it as a variable called `survey_raw`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bcbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "survey_raw = pd.read_csv(data_file)\n",
    "\n",
    "# Display the first lines of content of the DataFrame\n",
    "survey_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce49ab8",
   "metadata": {
    "id": "1ce49ab8"
   },
   "source": [
    "You can read more about how to import different formats of data into Python [here](https://images.datacamp.com/image/upload/v1676302004/Marketing/Blog/Importing_Data_Cheat_Sheet.pdf).\n",
    "\n",
    "Looking at the table above, we see the [`head()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) returns the first five rows of the DataFrame. You can specify how many rows you want by passing the number as a parameter to the function (e.g. `head(10)`). The [`tail()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html) works in a similar way for the last rows of the table.\n",
    "\n",
    "Look now at the number of columns. At the bottom of the table it says there are 84 columns, but only the first and last 10 are shown by default. If we want to see all columns, it is possible to change the settings of Pandas to achieve this. Alternatively, we can use the `itables` package that will display Pandas DataFrames as interactive tables, allowing us to scroll, as well as sort, paginate, and filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0183905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itables import show\n",
    "\n",
    "show(survey_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af02e5a",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have seen how to import a dataset, we can start the process of “cleaning” it.\n",
    "Cleaning, processing, wrangling, tidying, etc are all synonyms you may have heard being used for this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdbe7a",
   "metadata": {
    "id": "5fbdbe7a"
   },
   "source": [
    "<a id='semantics'></a>\n",
    "## Data semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09e239",
   "metadata": {
    "id": "ef09e239"
   },
   "source": [
    "A dataset is a collection of *values*, usually either numbers (if quantitative) or strings (if qualitative). Every value belongs to a **variable** and an **observation**:\n",
    "\n",
    "-   A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units;\n",
    "\n",
    "-   An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.\n",
    "\n",
    "A dataset is messy or clean/tidy depending on how rows and columns are matched up with observations and variables. In **tidy data**:\n",
    "\n",
    "-   Each variable is a column;\n",
    "\n",
    "-   Each observation is a row;\n",
    "\n",
    "-   Each cell is a single measurement.\n",
    "\n",
    "<img src=\"https://github.com/Sydney-Informatics-Hub/EDAinPy/blob/tomm-updates/fig/tidy_data.png?raw=1\" width=\"70%\"/>\n",
    "\n",
    "**Messy data** is any other arrangement of the data. There are common problems that can arise when dealing with messy datasets. There are a number of actions you can take on a dataset to tidy the data depending on the problem.\n",
    "These include: *filtering*, *transforming*, *modifying variables*, *aggregating the data*, and *sorting the order of the observations*. We will see some of this later in this tutorial.\n",
    "\n",
    "Here is a [link](https://byuidatascience.github.io/python4ds/tidy-data.html) to some material you can use to better understand how to tidy messy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf254d9",
   "metadata": {
    "id": "3bf254d9"
   },
   "source": [
    "***\n",
    "### ▶️ **It's your turn!** ###\n",
    "\n",
    "Have a look at your survey_raw object.\n",
    "Is your dataset tidy or messy?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "We can gain a lot of information by just viewing the data itself:\n",
    "\n",
    "```python\n",
    "survey_raw.head()\n",
    "```\n",
    "</details>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234caa1",
   "metadata": {
    "id": "0234caa1"
   },
   "source": [
    "<a id='cleaning'></a>\n",
    "## Inspection and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138fc686",
   "metadata": {
    "id": "138fc686"
   },
   "source": [
    "Data cleaning is an iterative process:\n",
    "\n",
    "-   We won't get everything right the first time around;\n",
    "-   We might improve or find a better way;\n",
    "-   There isn't always a clear place to start, but we have to start somewhere.\n",
    "\n",
    "<img src=\"https://github.com/Sydney-Informatics-Hub/EDAinPy/blob/tomm-updates/fig/research_framework.png?raw=1\" width=\"50%\"/>\n",
    "\n",
    "One good place to start is getting an overview of the dataset and check for *data-type constraints*. You can do this by calling the [`info()` function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) on a pandas DataFrame.\n",
    "\n",
    "> DATA-TYPE CONSTRAINTS: Make sure numbers are stored as numerical data types. A date should be stored as a date object, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90457d12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90457d12",
    "outputId": "aaabd254-ddc7-4f16-ecba-7d141a98175e"
   },
   "outputs": [],
   "source": [
    "survey_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9e597",
   "metadata": {
    "id": "ada9e597"
   },
   "source": [
    "This function provides a concise summary of the DataFrame. Here's what each part of the output means:\n",
    "\n",
    "- `<class 'pandas.core.frame.DataFrame'>`: survey_raw is a pandas DataFrame.\n",
    "\n",
    "- `RangeIndex: 89184 entries, 0 to 89183`: your DataFrame has 89,184 rows. The index for these rows ranges from 0 to 89,183.\n",
    "\n",
    "- `Data columns (total 84 columns)`: your DataFrame has 84 columns.\n",
    "\n",
    "The next section lists each column in your DataFrame, along with the number of non-null entries and the data type for each column. For example,\n",
    " \n",
    " ` 0   ResponseId                           89184 non-null  int64` \n",
    " \n",
    " tells you that the first (or rather zeroth) column is called `ResponseId`, it has 89,184 non-null entries, and is of type `int64`.\n",
    "\n",
    "- `dtypes: float64(3), int64(1), object(80)`: your DataFrame has 3 columns of type `float64`, 1 column of type `int64`, and 80 columns of type `object`.\n",
    "\n",
    "- `memory usage: 57.2+ MB`: your DataFrame is using over 57.2 megabytes of memory.\n",
    "\n",
    "From this output, you can see that most of your columns are of type object, which usually means they contain strings or pandas could not infer a more specific data type. You also have some missing values in many columns, as the number of non-null entries is less than the total number of rows for those columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941a8d3",
   "metadata": {
    "id": "1941a8d3"
   },
   "source": [
    "### Unique identifiers\n",
    "\n",
    "The first variable `ResponseId` looks like a unique identifier. Each subject should have their own ID such that all data are properly attributed to that subject.\n",
    "\n",
    "> UNIQUE CONSTRAINT: a particular observation unit identifiable and distinct from all other observational units\n",
    "\n",
    "Often, it can be helpful to have a way of recording the source of each row in our original dataset (in this case, our subject), so we can trace the data from this row as we work with the data. We can use the [`unique()` method](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) on a column to obtain an array that only contains its unique values. The code below checks to see if the length of the unique values in the column `ResponseId` is the same as the length of the dataset. If it is, it will return `True` and we can be confident it is a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4affa8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac4affa8",
    "outputId": "693e0530-2a6c-4754-f163-9cff2c3e05c2"
   },
   "outputs": [],
   "source": [
    "# Check if 'ResponseId' uniquely identifies each row\n",
    "len(survey_raw) == len(survey_raw['ResponseId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577f1ab",
   "metadata": {
    "id": "2577f1ab"
   },
   "source": [
    "We can accomplish the same by calling the [`duplicated()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html) on the column. This method returns a series of `True` and `False` values that denote duplicate rows. The default behaviour of this method is to mark duplicates as `True` except for the first occurance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac8a55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65ac8a55",
    "outputId": "3d91bb53-bfa3-406d-f5df-0bb6916e3467"
   },
   "outputs": [],
   "source": [
    "# check for duplicate values in ResponseId column\n",
    "duplicate_values = survey_raw['ResponseId'].duplicated()\n",
    "\n",
    "# show result\n",
    "duplicate_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497f5f5",
   "metadata": {},
   "source": [
    "Because `True` also has the numeric value `1` and `False` has the value `0`, we can sum over this series to get a count of how many duplicated values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "sum(duplicate_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59a45f",
   "metadata": {
    "id": "2f59a45f"
   },
   "source": [
    "### Cleaning column names\n",
    "\n",
    "When we look at the list of column names, we can see that they are all in the *CamelCase* format, that is, each new word starts with an uppercase letter with no spaces. Sometimes the data sets we work with don't have this consistency, which makes them more difficult to work with - how do you keep track of which variable is written in which format? At other times, we may want to change the way the column name is formatted. For example, in Python, the naming convention for variable names is that they are written in *snake_case* (that is, lowercase and separate words with underscores).\n",
    "\n",
    "If we want to uniformly change the column names so they have a particular format, it would be painful to change them all individually. Instead, we can use the [`clean_columns` function](https://aeturrell.github.io/skimpy/reference/clean_columns.html) that can be found in the `skimpy` library. If we apply it to our DataFrame, we can see that the column names are converted from CamelCase to snake_case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2bff9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3d2bff9",
    "outputId": "24dae3d6-d305-4265-88e2-f07bec704d8f"
   },
   "outputs": [],
   "source": [
    "from skimpy import clean_columns #clean_columns comes from skimpy\n",
    "\n",
    "survey_cleaned = clean_columns(survey_raw)\n",
    "survey_cleaned.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a6674",
   "metadata": {},
   "source": [
    "While snake_case is the default style returned by `clean_columns`, there is the option to select other styles, as detailed in the [function documentation](https://aeturrell.github.io/skimpy/reference/clean_columns.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ce982",
   "metadata": {
    "id": "b85ce982"
   },
   "source": [
    "### Reducing categories and renaming variables\n",
    "\n",
    "When we had a look at our data, you might have noticed that some variables have many levels. We may want to collapse these levels into fewer categories for several reasons:\n",
    "\n",
    "- Collapsing levels can simplify the analysis and interpretation of results, especially when dealing with a large number of categories;\n",
    "- In predictive modelling, reducing the number of levels in categorical variables can help improve model performance by reducing the complexity of the model;\n",
    "- Collapsing infrequent levels into broader categories can help address issues of sparsity, where certain levels have very few observations, which may lead to unreliable estimates.\n",
    "\n",
    "For example, the `coding_activities` variable has many levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2480db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "4a2480db",
    "outputId": "f3dc1f39-0247-4c0b-fa2e-786c495a1d02"
   },
   "outputs": [],
   "source": [
    "# Count occurrences of 'coding_activities'\n",
    "show(survey_cleaned['coding_activities'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf096e",
   "metadata": {
    "id": "cccf096e"
   },
   "source": [
    "But what does this variable represent?\n",
    "Some variable names are not easily understandable.\n",
    "Details on the questions asked in the survey are provided in the document `survey_result_schema.csv`. You can read this document in the same way you read in the stackoverflow survey dataset, with the `read_csv()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81c919",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cf81c919",
    "outputId": "426212b2-25be-4768-a286-421aeba8ca12"
   },
   "outputs": [],
   "source": [
    "survey_schema = pd.read_csv(\"survey_results_schema.csv\")\n",
    "\n",
    "# Clean column names\n",
    "survey_schema_cleaned = survey_schema.clean_names(column_names='qname', case_type='snake', axis=None)\n",
    "\n",
    "# explore the question variable\n",
    "show(survey_schema_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005620a",
   "metadata": {
    "id": "a005620a"
   },
   "source": [
    "Now that we know that our `coding_activities` variable is related to whether the participants code as a hobby or for things work-related, we can rename this column as `coding_as_hobby`.\n",
    "\n",
    "Similarly, the column `converted_comp_yearly` stands for the annual salary of the respondents in USD. We can change the name of column to `salary_usd` for easier understanding.\n",
    "\n",
    "Below we create a dictionary called `lookup` that maps old names to new names for several columns. We can then pass this dictionary to the [`rename()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) of the DataFrame to get a new DataFrame with the changed column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e549638",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e549638",
    "outputId": "1c33b111-a0f4-4ae7-e1cb-01623c006c90"
   },
   "outputs": [],
   "source": [
    "# Columns to be renamed\n",
    "lookup = {\n",
    "    \"converted_comp_yearly\": \"salary_usd\",\n",
    "    \"main_branch\": \"profession\",\n",
    "    \"coding_activities\": \"coding_as_hobby\",\n",
    "    \"years_code_pro\": \"years_professional_coding\",\n",
    "    \"org_size\": \"company_size\",\n",
    "    \"work_exp\": \"years_work_exp\",\n",
    "    \"i_cor_pm\": \"manager_role\"\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "survey_renamed = survey_cleaned.rename(columns=lookup)\n",
    "\n",
    "# Check the new list of column names\n",
    "survey_renamed.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deff9f3",
   "metadata": {
    "id": "0deff9f3"
   },
   "source": [
    "Let's go back to our `coding_as_hobby` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea263f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "adea263f",
    "outputId": "2121a25c-ea86-4ea0-ce3a-d405efbe82ed"
   },
   "outputs": [],
   "source": [
    "# Count occurrences of 'coding_activities'\n",
    "show(survey_renamed['coding_as_hobby'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda1de2",
   "metadata": {
    "id": "4cda1de2"
   },
   "source": [
    "There are lots of values and in this case it would be much easier to make this variable binary:\n",
    "\n",
    "- `yes`- the respondent codes outside of work (as hobby);\n",
    "- `no`- the respondent doesn’t code outside of work.\n",
    "\n",
    "When we want to do something like this, we can define a Python function that calculates the conversion. We can then use the [`apply() method`](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) to apply this function to a column.\n",
    "\n",
    "Below we first copy the `survey_renamed` DataFrame into a new version called `survey_hobby`. We apply the `map_hobby` function to the `coding_as_hobby` variable and assign the result into a new variable called `coding_as_hobby_clean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18153196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map value to \"yes\" or \"no\"\n",
    "def map_hobby(value):\n",
    "    if pd.isna(value): # Keep NaN values as is\n",
    "        return value\n",
    "    if isinstance(value,str): # Check if the value is a string\n",
    "        if \"Hobby\" in value:\n",
    "            return \"yes\"\n",
    "    return \"no\"\n",
    "\n",
    "# Make a new copy of our dataframe\n",
    "survey_hobby = survey_renamed.copy()\n",
    "\n",
    "# Apply our function and save the result as a new variable\n",
    "survey_hobby['coding_as_hobby_clean'] = survey_hobby['coding_as_hobby'].apply(map_hobby)\n",
    "\n",
    "# Look at the result\n",
    "survey_hobby['coding_as_hobby_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e738253",
   "metadata": {},
   "source": [
    "Because the data in this variable only takes on a limited number of possible values, we can explicitly make this into a [`Categorical`](https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8160d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_hobby['coding_as_hobby_clean'] = pd.Categorical(survey_hobby['coding_as_hobby_clean'])\n",
    "\n",
    "# Look at the result now\n",
    "survey_hobby['coding_as_hobby_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e986c1",
   "metadata": {
    "id": "f8e986c1"
   },
   "source": [
    "How can we visualise how many respondents engage in coding as a hobby?\n",
    "Let's compare how the `coding_as_hobby` and `coding_as_hobby_clean` variables are visualised in a bar plot. We will use the [`countplot()` function](https://seaborn.pydata.org/generated/seaborn.countplot.html) from the `seaborn` plotting library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666c650",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7666c650",
    "outputId": "194c3afa-01d1-4694-e854-1ebd708ffb9c"
   },
   "outputs": [],
   "source": [
    "# Plot with matplotlib and seaborn libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#raw variable\n",
    "sns.countplot(data=survey_hobby, x='coding_as_hobby')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Coding as Hobby')\n",
    "plt.ylabel('Count')\n",
    "sns.despine()  # To achieve a classic-looking theme without gridlines\n",
    "plt.show()\n",
    "\n",
    "#cleaned variable\n",
    "sns.countplot(data=survey_hobby, x='coding_as_hobby_clean')\n",
    "plt.xlabel('Coding as Hobby')\n",
    "plt.ylabel('Count')\n",
    "sns.despine()  # To achieve a classic-looking theme without gridlines\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71aefa7",
   "metadata": {
    "id": "c71aefa7"
   },
   "source": [
    "<a id='errors'></a>\n",
    "## In-record & cross-dataset errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57769c15",
   "metadata": {
    "id": "57769c15"
   },
   "source": [
    "These errors result from having two or more values in the same row or across datasets that contradict with each other.\n",
    "\n",
    "For example, if we have a dataset about the cost of living in cities. The total column must be equivalent to the sum of rent, transport, and food. Similarly, a child can’t be married. An employee’s salary can’t be less than the calculated taxes.\n",
    "\n",
    "The same idea applies to related data across different datasets.\n",
    "\n",
    "Looking at our dataset, you can notice that some respondents have provided conflicting information about their *employment status* (e.g. variable `employment` levels: `Employed, full-time;Not employed, but looking for work` or `Not employed, but looking for work;Not employed, and not looking for work`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5382e09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "a5382e09",
    "outputId": "8dc4c22c-49f7-4a89-c581-9d74c2f8ae38"
   },
   "outputs": [],
   "source": [
    "# Explore the 'employment' variable\n",
    "show(survey_hobby['employment'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d7cf3",
   "metadata": {
    "id": "b83d7cf3"
   },
   "source": [
    "As it is not possible to reach out to the respondents who provided such responses, we might *correct* them, if it’s clear what the correct response should be, or we may choose to *remove* these responses if they cannot be resolved. If you look at the `count` column, you can see that the number of respondents giving conflicting answers is less than 30. We can recode these respondent’s answers to the employment variable as `“unknown”` (**flagging**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1d103",
   "metadata": {
    "id": "a1a1d103"
   },
   "outputs": [],
   "source": [
    "# Define the function to clean the employment column\n",
    "def clean_employment(employment):\n",
    "    if pd.isna(employment):\n",
    "        return employment  # Keep NaN values as is\n",
    "    if \"Employed\" in employment and \"Not employed\" in employment:\n",
    "        return \"unknown\"\n",
    "    if \"but\" in employment and \"and\" in employment:\n",
    "        return \"unknown\"\n",
    "    return employment\n",
    "\n",
    "# Apply the function to create the employment_clean column\n",
    "survey_employment = survey_hobby.copy()\n",
    "survey_employment['employment_clean'] = survey_employment['employment'].apply(clean_employment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae147d7f",
   "metadata": {
    "id": "ae147d7f"
   },
   "source": [
    "We can now map each value to either `“employed”`, `“self-employed”`, `“student”`, `“retired”`, `“not_employed”` or `“unknown”`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7115e6d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7115e6d5",
    "outputId": "2ebe1ada-a6ec-46bb-ad6a-56141b3255b9"
   },
   "outputs": [],
   "source": [
    "# Define the function to further clean the employment_clean column\n",
    "def further_clean_employment(employment_clean):\n",
    "    if pd.isna(employment_clean):\n",
    "        return np.nan\n",
    "    if \"Employed\" in employment_clean:\n",
    "        return \"employed\"\n",
    "    if \"freelancer\" in employment_clean:\n",
    "        return \"self_employed\"\n",
    "    if \"Student\" in employment_clean:\n",
    "        return \"student\"\n",
    "    if \"Retired\" in employment_clean:\n",
    "        return \"retired\"\n",
    "    if \"Not employed\" in employment_clean:\n",
    "        return \"not_employed\"\n",
    "    if \"I prefer not to say\" in employment_clean:\n",
    "        return \"unknown\"\n",
    "    return employment_clean\n",
    "\n",
    "# Apply the function to create the further cleaned employment_clean column\n",
    "survey_employment['employment_clean'] = survey_employment['employment_clean'].apply(further_clean_employment)\n",
    "#convert to categorical data type\n",
    "survey_employment['employment_clean'] = pd.Categorical(survey_employment['employment_clean'])\n",
    "\n",
    "# Count the values, including NaN counts\n",
    "survey_employment['employment_clean'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf2aca",
   "metadata": {
    "id": "8dcf2aca"
   },
   "source": [
    "***\n",
    "ℹ️ **Covariation**\n",
    "\n",
    "Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualise the relationship between two or more variables. How you do that should depend on the type of variables involved.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad634d1",
   "metadata": {
    "id": "6ad634d1"
   },
   "source": [
    "You can visualise the **covariation** between the categorical variables `coding_as_hobby_clean` and `employment_clean` by counting the number of observations for each combination of levels of these variables. To do this we first apply the [`groupby()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) to group the data by these categories, then measure the size of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f124c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "56f124c1",
    "outputId": "c11d1038-6646-44a7-a9ce-f62c6d5c5343"
   },
   "outputs": [],
   "source": [
    "chart_dat = (survey_employment\n",
    "    .groupby(['employment_clean', 'coding_as_hobby_clean'])\n",
    "    .size()\n",
    "    .reset_index(name = 'n'))\n",
    "\n",
    "show(chart_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8990b4",
   "metadata": {},
   "source": [
    "Let's look at it as a bar chart using the seaborn [`barplot()` function](https://seaborn.pydata.org/generated/seaborn.barplot.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iX9_cI-cBmbi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "iX9_cI-cBmbi",
    "outputId": "765a9514-c723-4fb4-cdee-8e525de98bbf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a bar chart\n",
    "sns.barplot(x='employment_clean', y='n', hue='coding_as_hobby_clean', data=chart_dat)\n",
    "plt.xticks(rotation=70)\n",
    "plt.xlabel('Employment Status')\n",
    "plt.ylabel('Number of Respondents')\n",
    "plt.title('Coding as Hobby by Employment Status')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f2cad",
   "metadata": {},
   "source": [
    "And as a heatmap using the [`heatmap()` function](https://seaborn.pydata.org/generated/seaborn.heatmap.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cePTEcILGYqT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "cePTEcILGYqT",
    "outputId": "24f19119-541b-4add-bd55-c2c0908fe7fb"
   },
   "outputs": [],
   "source": [
    "# Create a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pd.pivot_table(chart_dat, values='n', index='employment_clean', columns='coding_as_hobby_clean'), cmap='crest')\n",
    "plt.xlabel('Coding as Hobby')\n",
    "plt.ylabel('Employment Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1257d4",
   "metadata": {
    "id": "5b1257d4"
   },
   "source": [
    "Covariation will appear as a strong correlation between specific x values and specific y values.\n",
    "From these visualisations, you can see that respondents that do not code as a hobby are mainly those that are employed or self-employed, and that employment status might influence whether individuals engage in coding as a hobby."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6a785",
   "metadata": {
    "id": "acc6a785"
   },
   "source": [
    "<a id='duplicates'></a>\n",
    "## Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efade304",
   "metadata": {
    "id": "efade304"
   },
   "source": [
    "Duplicates are data points that are repeated in your dataset. An example could be when two users have the same identity number. These values should be removed. You could also look for fully identical rows in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee8a42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "id": "47ee8a42",
    "outputId": "7403e48c-fb2a-4414-e526-b7969816bb2c"
   },
   "outputs": [],
   "source": [
    "# Test for fully identical rows\n",
    "survey_employment[survey_employment.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cea3c",
   "metadata": {
    "id": "045cea3c"
   },
   "source": [
    "<a id='standardise'></a>\n",
    "## Standardise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa636d",
   "metadata": {
    "id": "91fa636d"
   },
   "source": [
    "> REGULAR EXPRESSION PATTERNS: some text fields have to be in a certain pattern. For example, phone numbers may be required to have the pattern (999) 999--9999.\n",
    "\n",
    "For each variable that we expect to be in a standardised format, we should:\n",
    "\n",
    "-   test that all provided values for this variable are indeed in this format;\n",
    "-   encode/store it in this format.\n",
    "\n",
    "You might have noticed that, in our original dataset, we had 2 numerical variables, `comp_total` (the annual salary in the currency the respondent use day-to-day) and `salary_usd` (the annual salary converted into USD dollars). In order to be able to make predictions and comparisons, we will consider only the `salary_usd` variable to perform our analysis.\n",
    "\n",
    "There are different ways we can quickly inspect our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebd072",
   "metadata": {
    "id": "f0ebd072"
   },
   "source": [
    "**[Skimpy](https://github.com/aeturrell/skimpy)** is a light weight tool that provides summary statistics about variables in pandas data frames within the console or your interactive Python window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c3cde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b96c3cde",
    "outputId": "65aee0a9-877e-4e34-9a6f-373ce4c9ed8b"
   },
   "outputs": [],
   "source": [
    "from skimpy import skim\n",
    "\n",
    "skim(survey_employment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74201a54",
   "metadata": {
    "id": "74201a54"
   },
   "source": [
    "Each section summarizes variables of the same type. Numerical variables also include histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741c9b1",
   "metadata": {
    "id": "4741c9b1"
   },
   "source": [
    "The output from `skim()` separately summarizes categorical and continuous variables. For continuous variables you get information about the mean and median (**p50**) column. You know what the range of the variable is (**p0** is the minimum value, **p100** is the maximum value for continuous variables). You also get a measure of variability with the standard deviation (**sd**). It even quantifies the number of missing values (**NA**) and shows you the distribution or shape of each variable (**hist**)!\n",
    "\n",
    "If we take a look closer we can see that `years_professional_coding` is a categorical variable. Let’s explore this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1386b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "7e1386b1",
    "outputId": "ac6d5597-e2d0-47b0-ba0a-1662f4e0ed54"
   },
   "outputs": [],
   "source": [
    "#list all the possible values for years_professiona_coding variable\n",
    "show(survey_employment['years_professional_coding'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0857b0f",
   "metadata": {
    "id": "b0857b0f"
   },
   "source": [
    "It looks like two values are character strings instead of single numbers (`“Less than 1 year”`, `“More than 50 years”`). Let’s standardise these values so that the `years_professional_coding` variable transforms into a numerical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad63b12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "9ad63b12",
    "outputId": "74283e2a-ec58-4878-b030-8a475dd7cc09"
   },
   "outputs": [],
   "source": [
    "# Define the function to convert character values to numbers and handle special values\n",
    "def clean_years(years):\n",
    "    if years == \"Less than 1 year\":\n",
    "        return int(0)\n",
    "    elif years == \"More than 50 years\":\n",
    "        return int(51)\n",
    "    elif isinstance(years, str) and years.isdigit(): # values are strings that represent numbers\n",
    "        return int(years)\n",
    "    else:\n",
    "        return years\n",
    "\n",
    "# Apply the function to the 'years_professional_coding' column\n",
    "survey_prof = survey_employment.copy()\n",
    "survey_prof['years_professional_coding_clean'] = survey_prof['years_professional_coding'].apply(clean_years)\n",
    "\n",
    "\n",
    "# Display the cleaned variable\n",
    "show(survey_prof[['years_professional_coding', 'years_professional_coding_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a3804",
   "metadata": {
    "id": "5e7a3804"
   },
   "source": [
    "***\n",
    "### ▶️ **It's your turn!** ###\n",
    "\n",
    "`years_code` is reported as a categorical variables. Can you convert it to be numeric?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "#list all the possible values for years_professiona_coding variable\n",
    "show(survey_prof['years_code'].value_counts(dropna=False))\n",
    "#create a copy of the survey_prof dataframe called survey_code\n",
    "survey_code = survey_prof.copy()\n",
    "#apply the clean_years functions to create a new variable called years_code_clean\n",
    "survey_code['years_code_clean'] = survey_code['years_code'].apply(clean_years)\n",
    "#compare before and after\n",
    "show(survey_code[['years_code', 'years_code_clean']])\n",
    "```\n",
    "</details>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889f562",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "1889f562",
    "outputId": "fe7c74cd-8f94-4503-935a-091da679410d"
   },
   "outputs": [],
   "source": [
    "#list all the possible values for years_professiona_coding variable\n",
    "show(survey_prof['years_code'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864d2a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "c864d2a1",
    "outputId": "1fe5dc33-af29-43ed-ce49-f25bff422846"
   },
   "outputs": [],
   "source": [
    "survey_code = survey_prof.copy()\n",
    "survey_code['years_code_clean'] = survey_code['years_code'].apply(clean_years)\n",
    "show(survey_code[['years_code', 'years_code_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1177d",
   "metadata": {
    "id": "5ec1177d"
   },
   "source": [
    "<a id='categorical'></a>\n",
    "## Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000dc6bc",
   "metadata": {
    "id": "000dc6bc"
   },
   "source": [
    "**Categoricals** are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales. In our dataset, `employment_clean` and `coding_as_hobby_clean` are categorical variables. We converted them to categorical type using `pd.Categorical()`. In those cases, we used the default behavior:\n",
    "\n",
    "- Categories are inferred from the data;\n",
    "- Categories are unordered.\n",
    "\n",
    "In contrast to statistical categorical variables, categorical data might have an order (e.g. ‘strongly agree’ vs ‘agree’ or ‘first observation’ vs. ‘second observation’), but numerical operations (additions, divisions, …) are not possible.\n",
    "\n",
    "All values of categorical data are either in categories or `np.nan`. Order is defined by the order of categories, not lexical order of the values. Internally, the data structure consists of a categories array and an integer array of codes which point to the real value in the categories array.\n",
    "\n",
    "The categorical data type is useful in the following cases:\n",
    "\n",
    "- A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory;\n",
    "\n",
    "- The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order;\n",
    "\n",
    "- As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cb90c",
   "metadata": {
    "id": "ea9cb90c"
   },
   "source": [
    "If a categorical variable represents an ordinal scale (i.e., categories have a meaningful order), we should preserve this ordinality during data preprocessing. The variable `company_size` is a categorical ordinal variable, but it's reported as a string (skim output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de55907",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7de55907",
    "outputId": "834056e5-3fc9-464b-86ce-cc6fff00090a"
   },
   "outputs": [],
   "source": [
    "# List all the possible values\n",
    "survey_code['company_size'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f57c10",
   "metadata": {
    "id": "e8f57c10"
   },
   "source": [
    "Let’s convert `company_size` to a categorical variable by specifying the different categories and setting `order=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ace66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc7ace66",
    "outputId": "3eea7e90-1fdc-47c6-f1f5-b2837f3258f1"
   },
   "outputs": [],
   "source": [
    "# Define the mapping for the new categories\n",
    "size_mapping_company = {\n",
    "    \"Just me - I am a freelancer, sole proprietor, etc.\": \"1\",\n",
    "    \"2 to 9 employees\": \"2-9\",\n",
    "    \"10 to 19 employees\": \"10-19\",\n",
    "    \"20 to 99 employees\": \"20-99\",\n",
    "    \"100 to 499 employees\": \"100-499\",\n",
    "    \"500 to 999 employees\": \"500-999\",\n",
    "    \"1,000 to 4,999 employees\": \"1,000-4,999\",\n",
    "    \"5,000 to 9,999 employees\": \"5,000-9,999\",\n",
    "    \"10,000 or more employees\": \">10,000\",\n",
    "    \"I don’t know\": \"unknown\"\n",
    "}\n",
    "\n",
    "# Create a new column with the cleaned company sizes\n",
    "survey_company = survey_code.copy()\n",
    "survey_company['company_size_clean'] = survey_company['company_size'].map(size_mapping_company)\n",
    "\n",
    "survey_company['company_size_clean'] = pd.Categorical(survey_company['company_size_clean'], categories=[\"1\",\n",
    "                                                                                                        \"2-9\",\n",
    "                                                                                                        \"10-19\",\n",
    "                                                                                                        \"20-99\",\n",
    "                                                                                                        \"100-499\",\n",
    "                                                                                                        \"500-999\",\n",
    "                                                                                                        \"1,000-4,999\",\n",
    "                                                                                                        \"5,000-9,999\",\n",
    "                                                                                                        \">10,000\",\n",
    "                                                                                                        \"unknown\"],\n",
    "                                                                                                        ordered=True)\n",
    "\n",
    "# Examine the new levels\n",
    "survey_company['company_size_clean'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eea982",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "57eea982",
    "outputId": "b88c867a-5a5b-4a85-fb8c-cce5f5eae4c1"
   },
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "pd.crosstab(survey_company['company_size'], survey_company['company_size_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200c343",
   "metadata": {
    "id": "d200c343"
   },
   "source": [
    "If there were new missing values, then you would know there was a mistake in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd230e",
   "metadata": {
    "id": "5efd230e"
   },
   "source": [
    "***\n",
    "### ▶️ **It's your turn!** ###\n",
    "\n",
    "`age` is reported here in age ranges and is a categorical variables that is ordinal. In the skim() output, it is defined as a string. Can you convert it into a category?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# List all the possible values\n",
    "survey_company['age'].value_counts(dropna=False)\n",
    "# Define the mapping for the new categories\n",
    "size_mapping_age = {\n",
    "    \"Under 18 years old\" : \"<18\",     \n",
    "    \"18-24 years old\" : \"18-24\",\n",
    "    \"25-34 years old\" : \"25-34\",\n",
    "    \"35-44 years old\" : \"35-44\",\n",
    "    \"45-54 years old\" : \"45-54\",\n",
    "    \"55-64 years old\" : \"55-64\",\n",
    "    \"65 years or older\" : \">65\",\n",
    "    \"Prefer not to say\" : \"unknown\"\n",
    "}\n",
    "#create a new dataframe survey_age\n",
    "survey_age = survey_company.copy()\n",
    "#create a new column age_clean and apply the mapping function\n",
    "survey_age['age_clean'] = survey_age['age'].map(size_mapping_age)\n",
    "#convert to category\n",
    "survey_age['age_clean'] = pd.Categorical(survey_age['age_clean'], categories=[\"<18\",\n",
    "                                                                              \"18-24\",\n",
    "                                                                              \"25-34\",\n",
    "                                                                              \"35-44\",\n",
    "                                                                              \"45-54\",\n",
    "                                                                              \"55-64\",\n",
    "                                                                              \">65\",\n",
    "                                                                              \"unknown\"],\n",
    "                                                                              ordered=True)\n",
    "#inspect\n",
    "survey_age['age_clean'].unique()\n",
    "# Compare before and after\n",
    "show(pd.crosstab(survey_age['age'], survey_age['age_clean']))\n",
    "```\n",
    "</details>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe190721",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe190721",
    "outputId": "1283a5bc-5904-4181-fe11-70e55d2c9ce8"
   },
   "outputs": [],
   "source": [
    "# List all the possible values\n",
    "survey_company['age'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74955b64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74955b64",
    "outputId": "3eb13647-2c87-4762-b28f-8b779fa9a5b1"
   },
   "outputs": [],
   "source": [
    "size_mapping_age = {\n",
    "    \"Under 18 years old\" : \"<18\",\n",
    "    \"18-24 years old\" : \"18-24\",\n",
    "    \"25-34 years old\" : \"25-34\",\n",
    "    \"35-44 years old\" : \"35-44\",\n",
    "    \"45-54 years old\" : \"45-54\",\n",
    "    \"55-64 years old\" : \"55-64\",\n",
    "    \"65 years or older\" : \">65\",\n",
    "    \"Prefer not to say\" : \"unknown\"\n",
    "}\n",
    "survey_age = survey_company.copy()\n",
    "survey_age['age_clean'] = survey_age['age'].map(size_mapping_age)\n",
    "survey_age['age_clean'] = pd.Categorical(survey_age['age_clean'], categories=[\"<18\",\n",
    "                                                                              \"18-24\",\n",
    "                                                                              \"25-34\",\n",
    "                                                                              \"35-44\",\n",
    "                                                                              \"45-54\",\n",
    "                                                                              \"55-64\",\n",
    "                                                                              \">65\",\n",
    "                                                                              \"unknown\"],\n",
    "                                                                              ordered=True)\n",
    "\n",
    "survey_age['age_clean'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c070a13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "5c070a13",
    "outputId": "42c36b66-5dac-44b3-84a5-0999b81f17bb"
   },
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "pd.crosstab(survey_age['age'], survey_age['age_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e857d8",
   "metadata": {
    "id": "d9e857d8"
   },
   "source": [
    "<a id='select_filter'></a>\n",
    "## Select Variables and Filter Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f0623",
   "metadata": {
    "id": "c48f0623"
   },
   "source": [
    "We often want to subset our data in some way before we do many of our analyses. We may want to do this for a number of reasons (e.g., easier cognitively to think about the data, the analyses depend on the subsetting). The code below show the two main ways to subset your data:\n",
    "\n",
    "-   *selecting* variables and\n",
    "-   *filtering* observations.\n",
    "\n",
    "Let's focus our analysis only on **developers**. We might want to to exclude from our analysis students, hobby programmers and former developers (`profession` variable). We might also want to exclude unemployed and retired respondents (`employment_clean` variable). Let's have a look at these two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75aa8eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f75aa8eb",
    "outputId": "068341a8-4545-41e5-90f4-416c4ab7c119"
   },
   "outputs": [],
   "source": [
    "# Count profession\n",
    "survey_age['profession'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f98da2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90f98da2",
    "outputId": "00c3b206-dbf5-4053-d54e-47311e89e2e1"
   },
   "outputs": [],
   "source": [
    "# Count employment_clean\n",
    "survey_age['employment_clean'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6674c",
   "metadata": {
    "id": "fbb6674c"
   },
   "source": [
    "We can filter (i.e. take out observations we don’t want) using boolean indexing to filter rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f9635",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea6f9635",
    "outputId": "0fd7c6c5-ae00-4eb4-fd40-9d926a8795e0"
   },
   "outputs": [],
   "source": [
    "# Apply first filter\n",
    "survey_filtered_profession = survey_age[survey_age['profession'] == 'I am a developer by profession']\n",
    "print(f\"Number of rows after profession filter: {survey_filtered_profession.shape[0]}\")\n",
    "\n",
    "# Apply second filter\n",
    "survey_filtered_employment = survey_filtered_profession[\n",
    "    (survey_filtered_profession['employment_clean'] == 'employed') |\n",
    "    (survey_filtered_profession['employment_clean'] == 'self_employed')\n",
    "]\n",
    "print(f\"Number of rows after employment filter: {survey_filtered_employment.shape[0]}\")\n",
    "\n",
    "# Final filtered data\n",
    "survey_filtered = survey_filtered_employment\n",
    "\n",
    "# A quicker way of doing this\n",
    "# By using `|` we are saying we want employed or self_employed respondents.\n",
    "# #In other words, if either are met, that observation will be kept:\n",
    "survey_filtered = survey_age[(survey_age['profession'] == 'I am a developer by profession') &\n",
    "                             ((survey_age['employment_clean'] == 'employed') |\n",
    "                              (survey_age['employment_clean'] == 'self_employed'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e749505",
   "metadata": {},
   "source": [
    "Alternatively, we can use the [`query()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html) on the DataFrame. This produces the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ca9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_filtered = survey_age.query('profession == \"I am a developer by profession\" & (employment_clean == \"employed\" | employment_clean == \"self_employed\")')\n",
    "\n",
    "print(f\"Number of rows after query: {survey_filtered.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1a3e5",
   "metadata": {
    "id": "c6b1a3e5"
   },
   "source": [
    "The first filter selected only developers and reduced the dataset from 89184 respondents to 67237. The second filter excluded unemployed, students and retired respondents and the dataset was further reduced to 63533 respondents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b93291",
   "metadata": {
    "id": "36b93291"
   },
   "source": [
    "We can also select only the variables that are of interest for us today. In pandas, you can remove rows or columns by specifying label names and corresponding axis, or by directly specifying index or column names.\n",
    "\n",
    "We might want to drop the variables `q120` (*You consent to providing your information to help us stay privacy compliant*), `survey_ease` (*How easy or difficult was this survey to complete?*), `survey_length` (*How do you feel about the length of the survey this year?*), `t_branch` (*Would you like to participate in the Professional Developer Series?*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809aeffa",
   "metadata": {
    "id": "809aeffa"
   },
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "columns_to_drop = [\"q_120\", \"survey_ease\", \"survey_length\", \"t_branch\"]\n",
    "survey_sel = survey_filtered.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c54e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "718c54e5",
    "outputId": "181e01df-8c76-43f9-b382-15232f2c9ae3"
   },
   "outputs": [],
   "source": [
    "# Display the number of columns before and after selection\n",
    "print(f\"Number of columns before selection: {survey_age.shape[1]}\")\n",
    "print(f\"Number of columns after selection: {survey_sel.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aca62d",
   "metadata": {
    "id": "01aca62d"
   },
   "source": [
    "<a id='unusual_values'></a>\n",
    "## Unusual values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394944c",
   "metadata": {
    "id": "b394944c"
   },
   "source": [
    "> RANGE CONSTRAINTS: typically, numbers or dates should fall within a certain range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d6a70",
   "metadata": {
    "id": "9a1d6a70"
   },
   "source": [
    "<a id='outliers'></a>\n",
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6eccb3",
   "metadata": {
    "id": "ce6eccb3"
   },
   "source": [
    "Cleaning your data essentially is getting to know your data in greater detail to be able to make informed decisions about problems. For example, some variables cannot take on negative values as their scale of measurement renders negative values non-sensical (e.g., yearly salary). Even a value of zero might not be a valid value to observe. Sometimes, variables might have a valid lower boundary, upper boundary, or both.\n",
    "\n",
    "> Outliers are observations that are unusual; data points that don’t seem to fit the pattern.\n",
    "\n",
    "Sometimes outliers are data entry errors, sometimes they are simply values at the extremes that happened to be observed in this data collection, and other times they suggest important new discoveries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b629782",
   "metadata": {
    "id": "3b629782"
   },
   "source": [
    "Let's use the **matplotlib** library to create a histogram with salary on the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32fa9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "3e32fa9b",
    "outputId": "c8ce9679-d30e-48f4-b84f-52cc45a09016"
   },
   "outputs": [],
   "source": [
    "# Plot histogram using matplotlib\n",
    "plt.hist(survey_sel['salary_usd'], bins=30, edgecolor='black')\n",
    "\n",
    "# Format x-axis\n",
    "tick = mtick.FuncFormatter(lambda x, pos: f'{x * 1e-6:,.0f}M')\n",
    "plt.gca().xaxis.set_major_formatter(tick)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.xlabel('salary_usd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db7fdc",
   "metadata": {
    "id": "66db7fdc"
   },
   "source": [
    "When you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take this distribution of the `salary_usd` variable. The only evidence of outliers is the unusually wide limits on the x-axis. There are so many observations in the common bins that the rare bins are very short, making it very difficult to see them (although maybe if you stare intently at the right of the bar you’ll spot something). To make it easy to see the unusual values, we need to zoom to small values of the y-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22bdab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "cf22bdab",
    "outputId": "5ad3d5f9-fdaa-4c16-f584-92c3927edc0c"
   },
   "outputs": [],
   "source": [
    "# Plot histogram using matplotlib\n",
    "plt.hist(survey_sel['salary_usd'], bins=30, edgecolor='black')\n",
    "\n",
    "# Format x-axis\n",
    "tick = mtick.FuncFormatter(lambda x, pos: f'{x * 1e-6:,.0f}M')\n",
    "plt.gca().xaxis.set_major_formatter(tick)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set y-axis limits\n",
    "plt.ylim(0, 20)\n",
    "\n",
    "plt.xlabel('salary_usd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6786033",
   "metadata": {
    "id": "c6786033"
   },
   "source": [
    "Outliers are innocent until proven guilty. With that being said, they should not be removed unless there is a good reason for that. For example, one can notice some weird, suspicious values that are unlikely to happen, and so decides to remove them. Though, they are worth investigating before removing. It is also worth mentioning that some models, like linear regression, are very sensitive to outliers. In other words, outliers might throw the model off from where most of the data lie.\n",
    "\n",
    "You might be tempted to drop the entire row with the unusual values, but one invalid value doesn’t imply that all the other values for that observation are also invalid. Additionally, if you have low quality data, by the time that you’ve applied this approach to every variable you might find that you don’t have any data left.\n",
    "\n",
    "Let’s have a better understanding on what’s going on with these values. The conversion rates could potentially introduce variability into the data. For example, if a currency is particularly strong against the USD, salaries in that currency might appear unusually high when converted to USD. Conversely, if a currency is weak against the USD, salaries might appear lower. Moreover, salaries might change significantly depending on the country respondends work in. Let’s have a closer look to how the salaries change depending on the country/continent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555e825",
   "metadata": {
    "id": "b555e825"
   },
   "source": [
    "But before doing this, let’s check country variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557391d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "2557391d",
    "outputId": "1c9d390a-ceb4-460c-a9b7-e296d5379d08"
   },
   "outputs": [],
   "source": [
    "show(survey_sel['country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77993441",
   "metadata": {
    "id": "77993441"
   },
   "source": [
    "There are a lot of countries with very few respondents. In the next steps, we will rename some country names for visualisation purposes, we will calculate the number of respondents by country and the country median salary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363d8ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "2363d8ef",
    "outputId": "898eba9d-d882-439d-8199-59f559470517"
   },
   "outputs": [],
   "source": [
    "# Make country names shorter\n",
    "survey_sel['country'] = survey_sel['country'].replace({\n",
    "    'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom',\n",
    "    'Venezuela, Bolivarian Republic of...': 'Venezuela',\n",
    "    'Iran, Islamic Republic of...': 'Iran',\n",
    "    'United States of America': 'United States',\n",
    "    'Congo, Republic of the...': 'Republic of the Congo',\n",
    "    'The former Yugoslav Republic of Macedonia': 'North Macedonia',\n",
    "    'United Republic of Tanzania': 'Tanzania',\n",
    "    'Syrian Arab Republic': 'Syria',\n",
    "    'Libyan Arab Jamahiriya': 'Libya',\n",
    "    'Republic of Moldova': 'Moldova',\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    'Brunei Darussalam': 'Brunei',\n",
    "    'Republic of Korea': 'South Korea',\n",
    "    \"Democratic People's Republic of Korea\": \"North Korea\",\n",
    "    'Hong Kong (S.A.R.)': 'Hong Kong',\n",
    "    'Russian Federation': 'Russia',\n",
    "})\n",
    "\n",
    "# Calculate number of respondents by country and median salary\n",
    "country_n_resp = (survey_sel.groupby('country')\n",
    "                  .agg(median_salary=('salary_usd', 'median'), n_resp=('country', 'size'))\n",
    "                  .reset_index())\n",
    "\n",
    "# Round median salary\n",
    "country_n_resp['median_salary'] = country_n_resp['median_salary'].round()\n",
    "\n",
    "# Sort by number of respondents\n",
    "show(country_n_resp.sort_values(by='n_resp', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7aacb",
   "metadata": {
    "id": "95f7aacb"
   },
   "source": [
    "We can now generate a boxplot using the seaborn [`boxplot()` function] to visualise annual salary distribution by country. We will limit ourselves to countries with at least 100 respondents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef7239",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "9aef7239",
    "outputId": "e067234b-ba4a-4b24-ad52-52b0efb00839"
   },
   "outputs": [],
   "source": [
    "# Add number of respondents and median salary information\n",
    "survey_resp = pd.merge(survey_sel, country_n_resp, on='country')\n",
    "\n",
    "# Display the first few rows\n",
    "show(survey_resp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29dde7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "9e29dde7",
    "outputId": "89be0e06-1ac2-43c8-9b3e-a659c51d144a"
   },
   "outputs": [],
   "source": [
    "# Filter countries with at least 100 respondents\n",
    "survey_resp_filtered = survey_resp.query('n_resp >= 100')\n",
    "\n",
    "# Get the order of countries by median salary\n",
    "order = survey_resp_filtered.groupby('country')['salary_usd'].median().sort_values(ascending=False).index\n",
    "\n",
    "# Plot boxplot\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.boxplot(data=survey_resp_filtered, y='country', x='salary_usd', fliersize=0.5, order=order)\n",
    "plt.xlim(0, 400000)\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8c30a",
   "metadata": {
    "id": "1aa8c30a"
   },
   "source": [
    "We can see that the United States has the highest median annual salary at ~150,000 USD, followed by Switzerland. The lower part of the graph is populated by Asian, South American and African countries.\n",
    "\n",
    "The median annual salary varies a lot depending on the country. We might want to treat outliers differently, for example by considering the salary distribution of each country or continent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff351b",
   "metadata": {
    "id": "bcff351b"
   },
   "source": [
    "***\n",
    "### ▶️ **It's your turn!** ###\n",
    "\n",
    "Let’s plot the variables `remote_work` or `manager_role` and look at how these might contribute to salary distribution in the different countries. For easier visualisation, select only respondent from `United States`, `Australia`, `Canada`, `Sweden`, `Italy` and `Singapore`.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Filter countries for selected countries\n",
    "selected_countries = ['United States', 'Australia', 'Canada', 'Sweden', 'Italy', 'Singapore']\n",
    "survey_resp_filtered = survey_resp[survey_resp['country'].isin(selected_countries)]\n",
    "\n",
    "# Get the order of countries by median salary\n",
    "order = survey_resp_filtered.groupby('country')['salary_usd'].median().sort_values(ascending=False).index\n",
    "\n",
    "# Plot boxplot for remote_work\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(data=survey_resp_filtered, y='country', x='salary_usd', hue='remote_work', fliersize=0.5, order=order)\n",
    "plt.xlim(0, 400000)\n",
    "plt.title('Salary Distribution by Country and Remote Work Status')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.show()\n",
    "\n",
    "# Plot boxplot for manager_role\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(data=survey_resp_filtered, y='country', x='salary_usd', hue='manager_role', fliersize=0.5, order=order)\n",
    "plt.xlim(0, 400000)\n",
    "plt.title('Salary Distribution by Country and Manager Role Status')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.show()\n",
    "```\n",
    "</details>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579959f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "3579959f",
    "outputId": "5e047429-a743-476f-c031-60c95a369b6c"
   },
   "outputs": [],
   "source": [
    "# Filter for selected countries\n",
    "selected_countries = ['United States', 'Australia', 'Canada', 'Sweden', 'Italy', 'Singapore']\n",
    "survey_resp_filtered = survey_resp[survey_resp['country'].isin(selected_countries)]\n",
    "\n",
    "# Or if using the query method:\n",
    "# survey_resp_filtered = survey_resp.query('country.isin(@selected_countries)') \n",
    "\n",
    "# Get the order of countries by median salary\n",
    "order = survey_resp_filtered.groupby('country')['salary_usd'].median().sort_values(ascending=False).index\n",
    "\n",
    "# Plot boxplot for remote_work\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(data=survey_resp_filtered, y='country', x='salary_usd', hue='remote_work', fliersize=0.5, order=order)\n",
    "plt.xlim(0, 400000)\n",
    "plt.title('Salary Distribution by Country and Remote Work Status')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b3d90",
   "metadata": {
    "id": "a95b3d90"
   },
   "source": [
    "Remote workers generally earn more than in-person or hybrid workers.\n",
    "\n",
    "While these seem to be the general trend, keep in mind that individual salaries can vary widely based on factors like industry, years of experience, level of education, and specific skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f91d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "932f91d9",
    "outputId": "d9803923-c83d-406e-f4cf-08e583605797"
   },
   "outputs": [],
   "source": [
    "# Plot boxplot for manager_role\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(data=survey_resp_filtered, y='country', x='salary_usd', hue='manager_role', fliersize=0.5, order=order)\n",
    "plt.xlim(0, 400000)\n",
    "plt.title('Salary Distribution by Country and Manager Role Status')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceabb5b",
   "metadata": {
    "id": "cceabb5b"
   },
   "source": [
    "Individual contributors (in orange) are employees who are not responsible for managing others. People managers (in blue) are employees who manage teams or departments. People managers generally earn more than individual contributors, but there can be exceptions, especially in countries with high salary variability like the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c907a7",
   "metadata": {
    "id": "31c907a7"
   },
   "source": [
    "<a id='missing'></a>\n",
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c9071",
   "metadata": {
    "id": "643c9071"
   },
   "source": [
    "Missing values represent the unknown so they are “contagious”: almost any operation involving an unknown value will also be unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55275caf",
   "metadata": {
    "id": "55275caf"
   },
   "source": [
    "We refer the missing data as **null**, **NaN**, or **NA** values in general.\n",
    "\n",
    "Let's identify all locations in the survey data that have null (missing or NaN) data values. We can use the `isnull` method to do this. The isnull method will compare each cell with a null value. If an element has a null value, it will be assigned a value of True in the output object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aeff83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "78aeff83",
    "outputId": "a1d50ea6-b90d-490a-cd05-9f7c7d2bef88"
   },
   "outputs": [],
   "source": [
    "show(survey_resp.isnull().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287bf21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "2287bf21",
    "outputId": "f8a4e318-1129-4843-c0f2-28faf58a8ef2"
   },
   "outputs": [],
   "source": [
    "# Check is there any missing values across each column\n",
    "show(survey_resp.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917db2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4917db2b",
    "outputId": "34c8c805-fa77-4891-948f-3045f9c4021b"
   },
   "outputs": [],
   "source": [
    "survey_resp.salary_usd.isnull().sum() ## how many missing values in salary_usd column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152b90b",
   "metadata": {
    "id": "7152b90b"
   },
   "source": [
    "If we look at the `salary_usd` column in the surveys data we notice that there are **NaN** (*Not a Number*) values. NaN values are undefined values that cannot be represented mathematically. Pandas, for example, will read an empty cell in a CSV or Excel sheet as a NaN. NaNs have some desirable properties: if we were to average the `salary_usd` column without replacing our NaNs, Python would know to skip over those cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89cdc0",
   "metadata": {
    "id": "7a89cdc0"
   },
   "source": [
    "Given the fact the missing values are unavoidable leaves us with the question of what to do when we encounter them. There are several different ways in which we can deal with NAs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817ea5b",
   "metadata": {
    "id": "7817ea5b"
   },
   "source": [
    "<a id='drop'></a>\n",
    "#### Drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0075579",
   "metadata": {
    "id": "b0075579"
   },
   "source": [
    "If missing values in a column are infrequent and occur randomly, the most straightforward approach is to remove/drop observations (rows) containing missing values. However, if a large portion of the column’s values are missing and this occurs randomly, it may be prudent to consider removing the entire column.\n",
    "\n",
    "Let’s check whether there are any missing values for the `remote_work` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded4155",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eded4155",
    "outputId": "7c873c54-93ca-4873-b96b-cb62cbd9d4a5"
   },
   "outputs": [],
   "source": [
    "survey_resp.remote_work.isnull().sum() ## how many missing values in remote_work column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512cd13",
   "metadata": {
    "id": "c512cd13"
   },
   "source": [
    "There are different ways we can inspect missing values in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bfdde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd2bfdde",
    "outputId": "65367b46-96c7-46a2-9ec4-f2327bd2f713"
   },
   "outputs": [],
   "source": [
    "# get count of missing values of a particular column by group\n",
    "# i.e., count of missing values of “salary_usd” column by group (“remote_work”)\n",
    "survey_resp.groupby('remote_work')['salary_usd'].apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf22d01",
   "metadata": {
    "id": "acf22d01"
   },
   "source": [
    "The [**missingno** library](https://github.com/ResidentMario/missingno) provides several easy-to-use tools for visualising missing data.\n",
    "The most basic plot is the bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff23fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "c6ff23fd",
    "outputId": "2146685c-3a64-449c-c9a8-cf7d8c6208ca"
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# Gives a bar chart of the missing values\n",
    "msno.bar(survey_resp[['remote_work',\n",
    "                         'industry',\n",
    "                         'years_work_exp',\n",
    "                         'salary_usd',\n",
    "                         'company_size_clean',\n",
    "                         'dev_type',\n",
    "                         'years_professional_coding_clean',\n",
    "                         'age_clean',\n",
    "                         'years_code_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99d015",
   "metadata": {
    "id": "0d99d015"
   },
   "source": [
    "Here you can immediately see that the `industry`,`years_work_exp` and `salary_usd` features are missing many values. A closer look also reveals that the features `remote_work`, `dev_type` and `years_code_clean` are missing a few values each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e9148",
   "metadata": {
    "id": "744e9148"
   },
   "source": [
    "The *nullity* matrix allows us to see the distribution of data across all columns in the whole dataset. It also shows a sparkline (or, in some cases, a striped line) that emphasizes rows in a dataset with the highest and lowest nullity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0fb0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "15e0fb0e",
    "outputId": "562f401e-2462-4361-f893-4c86682a0b9a"
   },
   "outputs": [],
   "source": [
    "msno.matrix(survey_resp[['remote_work',\n",
    "                         'industry',\n",
    "                         'years_work_exp',\n",
    "                         'salary_usd',\n",
    "                         'company_size_clean',\n",
    "                         'dev_type',\n",
    "                         'years_professional_coding_clean',\n",
    "                         'age_clean',\n",
    "                         'years_code_clean']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3caf72",
   "metadata": {
    "id": "bc3caf72"
   },
   "source": [
    "A final visualization you can use is the **heatmap**. This is slightly more complicated than the bar chart and the matrix plot. However, it can sometimes reveal interesting connections between missing values of different features.\n",
    "\n",
    "To get a heatmap, you can use the function `heatmap()` in the missingno library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7a458",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "85b7a458",
    "outputId": "50f1c47a-5665-498c-d4ba-186a42054f10"
   },
   "outputs": [],
   "source": [
    "# Gives a heatmap of how missing values are related\n",
    "msno.heatmap(survey_resp[['remote_work',\n",
    "                         'industry',\n",
    "                         'years_work_exp',\n",
    "                         'salary_usd',\n",
    "                         'company_size_clean',\n",
    "                         'dev_type',\n",
    "                         'years_professional_coding_clean',\n",
    "                         'age_clean',\n",
    "                         'years_code_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9f43d",
   "metadata": {
    "id": "bcc9f43d"
   },
   "source": [
    "First of all, notice that there are only 8 features present in the heatmap. This is because there are only 8 features that are missing values. All the other features are discarded from the plot (only `age_clean` in this case).\n",
    "\n",
    "To understand the heatmap, look at the value that corresponds to `company_size_clean` and `year_professional_coding_clean`. The value is 0.9. This means that there is a nearly perfect correspondence between missing values in `company_size_clean` and missing values in `year_professional_coding_clean` . You can also see this from the matrix plot you made before.\n",
    "\n",
    "The values in the heatmap range between -1 and 1. A value of -1 indicates a negative correspondence: A missing value in feature A implies that there is not a missing value in feature B.\n",
    "\n",
    "Finally, a value of 0 indicates that there is no obvious correspondence between missing values in feature A and missing values in feature B. This is the case for the `remote_work` feature.\n",
    "The `remote_work` variables has very few missing values. Given the sample size of our dataset, we might want to drop these missing values and we can do this with the `dropna()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619e3a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8619e3a5",
    "outputId": "f0be6068-f21f-45df-c372-40306172bccd"
   },
   "outputs": [],
   "source": [
    "# Drop rows where 'remote_work' is missing\n",
    "survey_remote = survey_resp.dropna(subset=['remote_work'])\n",
    "\n",
    "# Display the number of rows and columns before and after dropping NaN values\n",
    "print(f\"Number of rows before selection: {survey_resp.shape[0]}\")\n",
    "print(f\"Number of rows after selection: {survey_remote.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf1e64",
   "metadata": {
    "id": "16bf1e64"
   },
   "source": [
    "<a id='impute'></a>\n",
    "#### Impute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cff6a",
   "metadata": {
    "id": "777cff6a"
   },
   "source": [
    "> **Imputation** is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset.\n",
    "\n",
    "Some argue that filling in the missing values leads to a loss in information. That’s because saying that the data is missing is informative in itself, and the algorithm should know about it. This is particularly important when the missing data doesn’t happen at random. Plotting the missingness patterns across different variables, as we did before, can provide insights into whether missing data occurs systematically or at random.\n",
    "\n",
    "During the survey, maybe some people might have refused to answer a certain question. Maybe this is due to privacy concerns or confidentiality agreements with their employers.\n",
    "\n",
    "As the variable `salary_usd` seems to miss completely at random, we can think of replacing the NA with the most frequent occurrence of the variable, which is the mean if the variable has a normal distribution, or the median otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1cd8bf",
   "metadata": {
    "id": "6e1cd8bf"
   },
   "source": [
    "##### Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde435c",
   "metadata": {
    "id": "fdde435c"
   },
   "source": [
    "Being able to describe the shape of your variables is necessary during your descriptive analysis.\n",
    "\n",
    "When talking about the shape of one’s data, we’re discussing how the values (observations) within the variable are distributed. Often, we first determine how spread out the numbers are from one another (do all the observations fall between 1 and 10? 1 and 1000? -1000 and 10?). This is known as the **range** of the values. The range is described by the *minimum and maximum values taken by observations in the variable* and we can have a look at the range of the `salary_usd` variable using `min()` and `max()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a9458",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e1a9458",
    "outputId": "e7117e53-aa8c-499c-8d20-23dd8cc61080"
   },
   "outputs": [],
   "source": [
    "# Calculate the minimum and maximum of 'salary_usd'\n",
    "min_salary = survey_resp['salary_usd'].min()\n",
    "max_salary = survey_resp['salary_usd'].max()\n",
    "\n",
    "# Print the range\n",
    "print(f\"Range of 'salary_usd': {min_salary} - {max_salary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44bf4e5",
   "metadata": {
    "id": "e44bf4e5"
   },
   "source": [
    "Let's visualise the distribution of this data (i.e. how the data are spread out over the range) using the seaborn [`kdeplot()` (kernel density estimate) function](https://seaborn.pydata.org/generated/seaborn.kdeplot.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320a45b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "4320a45b",
    "outputId": "22b4e635-b477-4705-cde5-e667169e5d55"
   },
   "outputs": [],
   "source": [
    "# Create a density plot of 'salary_usd'\n",
    "sns.kdeplot(data=survey_remote, x='salary_usd')\n",
    "\n",
    "# Set the background to white\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1477a0e",
   "metadata": {
    "id": "d1477a0e"
   },
   "source": [
    "Here we see that the data are *skewed right*, given the shift in values away from the right, leading to a long right tail. Here, most of the values are at the lower end of the range.\n",
    "\n",
    "<img src=\"https://github.com/Sydney-Informatics-Hub/EDAinPy/blob/tomm-updates/fig/skewed_dist.png?raw=1\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a2eb7",
   "metadata": {
    "id": "b13a2eb7"
   },
   "source": [
    "Let’s replace the missing values in the `salary_usd` variable with the median value of `salary_usd` for each country that we had previously calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fc224",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c1fc224",
    "outputId": "1629882d-a856-41ef-a436-8d6a8dab4634"
   },
   "outputs": [],
   "source": [
    "# Copy the DataFrame\n",
    "survey_proc = survey_remote.copy()\n",
    "\n",
    "# Replace missing values in 'salary_usd' with the median value for each country\n",
    "survey_proc['salary_usd'] = survey_proc['salary_usd'].fillna(survey_proc['median_salary'])\n",
    "\n",
    "# Check how many missing values are left in the 'salary_usd' column\n",
    "len(survey_proc[pd.isnull(survey_proc.salary_usd)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c1e72",
   "metadata": {
    "id": "299c1e72"
   },
   "source": [
    "We still have 14 missing values in the `salary_usd` variable. What are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d721dbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "9d721dbc",
    "outputId": "63f15567-d6fd-4b71-ef77-38a0794eba14"
   },
   "outputs": [],
   "source": [
    "# Filter rows where 'salary_usd' is missing and select certain columns\n",
    "survey_proc_na_salary = survey_proc[survey_proc['salary_usd'].isna()][['country', 'salary_usd', 'median_salary', 'n_resp']]\n",
    "\n",
    "# Display the DataFrame\n",
    "show(survey_proc_na_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821148e",
   "metadata": {
    "id": "7821148e"
   },
   "source": [
    "These are those countries with less than 3 respondents. As there are no salary observations for these countries, it wasn’t possible to calculate the median salary. Depending on your main research question, you might want to drop these rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898499e",
   "metadata": {
    "id": "9898499e"
   },
   "source": [
    "<a id='relationship'></a>\n",
    "## Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc6343",
   "metadata": {
    "id": "8efc6343"
   },
   "source": [
    "Sometimes missing values don’t occur randomly. Let’s consider the `years_professional_coding_clean` variable. This variable represents the number of years a respondent has been coding professionally. Younger respondents or those who are new to the workforce might be more likely to have missing values in this variable because they haven’t been coding professionally for many years.\n",
    "\n",
    "To verify this, we could explore the relationship between `years_professional_coding_clean` and other variables such as `years_work_exp_clean`. If there’s a pattern where younger respondents or those with fewer years of working experience tend to have missing values in `years_professional_coding_clean`, it suggests that the missingness is not random but related to certain characteristics of the respondents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74fe67",
   "metadata": {
    "id": "ce74fe67"
   },
   "source": [
    "You’ve already seen one great way to visualise the covariation between two categorical variables. Let's now visualise covariation between two continuous variables by drawing a **scatterplot**. You can see covariation as a pattern in the points. For example, you can see a linear relationship between years of professional coding experience and years of working experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2uPlNgb9506B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "2uPlNgb9506B",
    "outputId": "d805e71b-7a9b-4a74-c31d-ca8ea7a9788b"
   },
   "outputs": [],
   "source": [
    "x_axis = 'years_professional_coding_clean'\n",
    "y_axis = 'years_work_exp'\n",
    "\n",
    "# Only plot finite (ie. non NaN) data\n",
    "survey_proc_plot = survey_proc.dropna(subset=[x_axis, y_axis])\n",
    "x_data = survey_proc_plot[x_axis]\n",
    "y_data = survey_proc_plot[y_axis]\n",
    "\n",
    "plt.scatter(x_data,y_data, s=10)\n",
    "plt.xlabel(x_axis)\n",
    "plt.ylabel(y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ea114",
   "metadata": {
    "id": "428ea114"
   },
   "source": [
    "Scatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform colours (as above). One way to fix the problem is by using the alpha argument to add transparency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d78b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "aa2d78b8",
    "outputId": "2fabea34-7678-4c4c-934b-ed2840bb8018"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_data, y_data, s=10, alpha=0.1)\n",
    "plt.xlabel(x_axis)\n",
    "plt.ylabel(y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904ccaf",
   "metadata": {
    "id": "1904ccaf"
   },
   "source": [
    "Let's fit a *trend* line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fcded3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "08fcded3",
    "outputId": "0763ec8a-05be-4a30-aabf-3211f4f536d6"
   },
   "outputs": [],
   "source": [
    "# Fit a linear trend line\n",
    "z = np.polyfit(x_data, y_data, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# Plot the scatter plot with the trend line\n",
    "plt.scatter(survey_proc[x_axis],survey_proc[y_axis], s=10, alpha=0.1)\n",
    "plt.plot(survey_proc[x_axis], p(survey_proc[x_axis]), color='red')\n",
    "plt.xlabel(x_axis)\n",
    "plt.ylabel(y_axis)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1f227",
   "metadata": {
    "id": "2bc1f227"
   },
   "source": [
    "This line indicates an upward trend, suggesting that as professional coding experience increases, overall work experience also increases. This trend line suggests a positive correlation between the two variables, meaning that individuals with more professional coding experience also tend to have more overall work experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6210ee39",
   "metadata": {
    "id": "6210ee39"
   },
   "source": [
    "<a id='correlation'></a>\n",
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6f4f2",
   "metadata": {
    "id": "1de6f4f2"
   },
   "source": [
    "We might want to look at the relationships between all of our continuous variables. A good way to do this is to use a visualization of **correlation**. Correlation is a measure of the relationship or interdependence of two variables. In other words, how much do the values of one variable change with the values of another. Correlation can be either positive or negative and it ranges from -1 to 1, with 1 and -1 indicating perfect correlation (1 being positive and -1 being negative) and 0 indicating no correlation. Let's first calculate a correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a9938",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "fb1a9938",
    "outputId": "1f43c3ca-fc72-4a6b-ad2e-2c6b246b1fde"
   },
   "outputs": [],
   "source": [
    "survey_num = survey_proc.select_dtypes(include=[np.number])\n",
    "corr = survey_num.corr(method = 'pearson')\n",
    "show(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c62839",
   "metadata": {
    "id": "a1c62839"
   },
   "source": [
    "We can now use this correlation matrix to generate a correlation plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953361b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "a953361b",
    "outputId": "d633c7ab-37ce-4486-ab08-c6d59dadcd08"
   },
   "outputs": [],
   "source": [
    "# Hide the upper triangle of the matrix\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "mask[corr==1] = False\n",
    "\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", linewidth=.5, vmin=-1, vmax=1, cmap='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b043af",
   "metadata": {
    "id": "79b043af"
   },
   "source": [
    "Positive values (in blue) indicate positive correlations, with numerical values displayed within each cell to quantify the strength of each correlation. There are some cells with zero value which are colored white indicating no observed correlation between those pairs of variables.\n",
    "\n",
    "We can see that the `years_professional_coding_clean` variable is positively correlated to the `years_work_exp` variable, as previously observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a833a",
   "metadata": {
    "id": "144a833a"
   },
   "source": [
    "<a id='verifying'></a>\n",
    "## Verifying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef142189",
   "metadata": {
    "id": "ef142189"
   },
   "source": [
    "After your data inspection and cleaning, you should verify correctness by re-inspecting the data. We have already discussed most of these things but here is a general summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a5b96",
   "metadata": {
    "id": "942a5b96"
   },
   "source": [
    "<a id='accuracy'></a>\n",
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7696a",
   "metadata": {
    "id": "a6f7696a"
   },
   "source": [
    "The degree to which the data is close to the true values. While defining all possible valid values allows invalid values to be easily spotted, it does not mean that they are accurate. A valid street address mightn’t actually exist. A valid person’s eye colour, say blue, might be valid, but not true (doesn’t represent the reality). Another thing to note is the difference between accuracy and precision. Saying that you live on the Earth may be true, but it's not precise. Where on Earth? Saying that you live at a particular street address is more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45228750",
   "metadata": {
    "id": "45228750"
   },
   "source": [
    "<a id='completeness'></a>\n",
    "### Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdbd46",
   "metadata": {
    "id": "3ebdbd46"
   },
   "source": [
    "The degree to which all required data is known. Missing data is going to happen for various reasons. One can mitigate this problem by questioning the original source if possible, say re-interviewing the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a767ae",
   "metadata": {
    "id": "f1a767ae"
   },
   "source": [
    "<a id='consistency'></a>\n",
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0bd90",
   "metadata": {
    "id": "86b0bd90"
   },
   "source": [
    "The degree to which the data is consistent, within the same data set or across multiple data sets.\n",
    "\n",
    "As we have already seen, inconsistency occurs when two values in the data set contradict each other. A valid age, say 10, mightn’t match with the marital status, say divorced. A customer is recorded in two different tables with two different addresses. Which one is true?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12f6d6",
   "metadata": {
    "id": "3c12f6d6"
   },
   "source": [
    "<a id='uniformity'></a>\n",
    "### Uniformity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74478f21",
   "metadata": {
    "id": "74478f21"
   },
   "source": [
    "The degree to which the data is specified using the same unit of measure. The weight may be recorded either in pounds or kilograms. The date might follow the USA format or the international standard. The currency may be in USD, or perhaps in yen. And so data must be converted to a single measure unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716623f",
   "metadata": {
    "id": "a716623f"
   },
   "source": [
    "[**YData Profiling**](https://docs.profiling.ydata.ai/) displays descriptive overview of the data sets, by showing the number of variables, observations, total missing cells, duplicate rows, memory used and the variable types. Then, it generates detailed analysis for each variable, class distributions, interactions, correlations, missing values, samples and duplicated rows, which you can observe by clicking each tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8de0",
   "metadata": {
    "id": "d7af8de0"
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Generate a data profiling report\n",
    "#report = ProfileReport(survey_proc, title='my data', minimal=True) #set minimal to True for less computational time\n",
    "#report.to_file(\"my_data_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002601ac",
   "metadata": {
    "id": "002601ac"
   },
   "source": [
    "Similarly, **SweetViz** is an open-source Python library, this is used for automated exploratory data analysis (EDA), it helps data analysts/scientists quickly generate beautiful & highly detailed visualizations. The output we get is a fully self-contained HTML application. The system built reports around quickly visualizing the target values & comparing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81097ec",
   "metadata": {
    "id": "b81097ec"
   },
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "#assuming we want to predict the variable salary_usd\n",
    "\n",
    "# Remove rows where 'salary_usd' is NaN\n",
    "#survey_report_salary = survey_proc.dropna(subset=['salary_usd'])\n",
    "\n",
    "# Then you can analyze the cleaned data\n",
    "#sv_report = sv.analyze(survey_report_salary, target_feat='salary_usd')\n",
    "\n",
    "# show the report in a form of an HTML file\n",
    "#sv_report.show_html('sv_report.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783fcc6",
   "metadata": {
    "id": "d783fcc6"
   },
   "source": [
    "<a id='reporting'></a>\n",
    "### Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6364c",
   "metadata": {
    "id": "c5d6364c"
   },
   "source": [
    "Reporting how healthy the data is, is equally important to cleaning. Be sure to include in your report (e.g. Jupyter notebook/Quarto) what the methods were used to clean the data and why because different methods can be better in different situations or with different data types. By documenting your process in this way, you provide valuable context and ensure that your data cleaning process is reproducible and transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204b22f",
   "metadata": {
    "id": "4204b22f"
   },
   "source": [
    "<a id='next'></a>\n",
    "## What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63f107",
   "metadata": {
    "id": "cf63f107"
   },
   "source": [
    "*Congratulations on completing your first exploratory data analysis (EDA)!*\n",
    "\n",
    "You've journeyed through the intricate landscape of the Stack Overflow dataset, exploring developers' demographics, professional experiences, and work dynamics.\n",
    "\n",
    "Now that our data is primed for analysis, the possibilities are endless.\n",
    "\n",
    "At the *Sydney Informatics Hub (SIH)*, we offer a myriad of avenues for further exploration and learning, ensuring that you're well-equipped to harness the power of data.\n",
    "\n",
    "Have a look at our *training calendar* [here](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub/workshops-and-training/training-calendar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95b1d1",
   "metadata": {
    "id": "4a95b1d1"
   },
   "source": [
    "***\n",
    "### ℹ️ **Additional information** ###\n",
    "\n",
    "There are a couple of other things that you will need to know and that are not covered in this tutorial:\n",
    "\n",
    "- **Joining dataframes**: this is common in many situations including health records and longitudinal studies.  Please have a look at this *[link](https://pandas.pydata.org/docs/user_guide/merging.html)* for additional resources;\n",
    "\n",
    "<img src=\"https://github.com/Sydney-Informatics-Hub/EDAinPy/blob/tomm-updates/fig/Joining.jpg?raw=1\" width=\"70%\"/>\n",
    "\n",
    "- **Reshaping dataframes**: converting your data from wide-to-long or from long-to-wide data formats. Please have a look at this *[link](https://pandas.pydata.org/docs/user_guide/reshaping.html)* for additional resources.\n",
    "\n",
    "<img src=\"https://github.com/Sydney-Informatics-Hub/EDAinPy/blob/tomm-updates/fig/reshape.png?raw=1\" width=\"70%\"/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e50fa-36d8-4d7b-8440-ecb7960bd7bb",
   "metadata": {
    "id": "fd7e50fa-36d8-4d7b-8440-ecb7960bd7bb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
